#
#def tokenizer(src) :
#    result = src.split()
#    print(result)
#
#
### test 
#f = open("inputTest.txt", "r")
#code = f.read()
#print(f.read())
#tokenizer(code)

class Tokenizer :

    def __init__(self, src, next, pos):
        self.src = src
        self.pos = 0
        computeNext()

    def  computeNext():
        print("compute")
    
    def peek():
        print("peek")

    def peek(str):
        print(str)

    def consume():
        print("consume")

    def consume(str):
        print(str)
        